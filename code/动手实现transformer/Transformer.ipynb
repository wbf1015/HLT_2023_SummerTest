{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "知识点：\n",
    "self-embedding\n",
    "position-embediing\n",
    "深入理解softmax\n",
    "encoder的mask\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 4], dtype=torch.int32)\n",
      "tensor([4, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import numpy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 关于word Embedding 以序列建模为例\n",
    "# 考虑source sentence 和 target sentence\n",
    "# 构建序列 序列的字符以其在词表中的索引的形式表示\n",
    "batch_size = 2\n",
    "\n",
    "# 模型的特征维度，可以理解为每一个字对应的Embedding后的向量维度\n",
    "model_dim = 8\n",
    "\n",
    "# 单词表大小\n",
    "max_num_src_words = 8\n",
    "max_num_tgt_words = 8\n",
    "\n",
    "# 序列最大长度\n",
    "max_src_seq_len = 5\n",
    "max_tgt_seq_len = 5\n",
    "max_position_len = 5\n",
    "\n",
    "# 随机生成batch中每个句子的长度\n",
    "# src_len = torch.randint(2,5,(batch_size,))\n",
    "# tgt_len = torch.randint(2,5,(batch_size,))\n",
    "src_len = torch.Tensor([2,4]).to(torch.int32)\n",
    "tgt_len = torch.Tensor([4,3]).to(torch.int32)\n",
    "\n",
    "print(src_len) #源长度的序列\n",
    "print(tgt_len) #目标长度的序列"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========词向量生成==========\n",
      "[tensor([1, 5]), tensor([6, 2, 2, 7])]\n",
      "[tensor([3, 5, 1, 3]), tensor([3, 7, 6])]\n",
      "==========序列填充==========\n",
      "[tensor([1, 5, 0, 0, 0]), tensor([6, 2, 2, 7, 0])]\n",
      "[tensor([3, 5, 1, 3, 0]), tensor([3, 7, 6, 0, 0])]\n",
      "==========向量拼接==========\n",
      "tensor([[1, 5, 0, 0, 0],\n",
      "        [6, 2, 2, 7, 0]])\n",
      "tensor([[3, 5, 1, 3, 0],\n",
      "        [3, 7, 6, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# 生成每个句子的具体内容，具体内容由单词索引表示\n",
    "src_seq = [torch.randint(1,max_num_src_words,(L,)) for L in src_len]\n",
    "tgt_seq = [torch.randint(1,max_num_tgt_words,(L,)) for L in tgt_len]\n",
    "print('==========词向量生成==========')\n",
    "print(src_seq)\n",
    "print(tgt_seq)\n",
    "\n",
    "# 还需要对每一个seq进行pad，来填充短句子\n",
    "for i in range(batch_size):\n",
    "    src_seq[i] = F.pad(src_seq[i],(0,max_src_seq_len-src_len[i]))\n",
    "    tgt_seq[i] = F.pad(tgt_seq[i],(0,max_tgt_seq_len-tgt_len[i]))\n",
    "print('==========序列填充==========')\n",
    "print(src_seq)\n",
    "print(tgt_seq)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    # unsqueeze()函数起升维的作用,参数表示在哪个地方加一个维度。\n",
    "    # 例如由[4]变成[1,4]\n",
    "    src_seq[i] = torch.unsqueeze(src_seq[i],0)\n",
    "    tgt_seq[i] = torch.unsqueeze(tgt_seq[i],0)\n",
    "src_seq = torch.cat(src_seq)\n",
    "tgt_seq = torch.cat(tgt_seq)\n",
    "print('==========向量拼接==========')\n",
    "print(src_seq)\n",
    "print(tgt_seq)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========构造嵌入表==========\n",
      "Parameter containing:\n",
      "tensor([[-1.4095e-01,  7.1341e-02,  5.8432e-01, -2.9907e-01, -1.7384e+00,\n",
      "         -4.7295e-01,  2.8286e-01, -3.2231e-01],\n",
      "        [-2.1713e+00,  5.4870e-03,  2.9440e-02,  2.2757e-01,  4.2504e-01,\n",
      "          1.7870e+00,  3.9924e-01, -1.4182e+00],\n",
      "        [-9.5577e-01, -3.4160e-01,  2.2756e-01, -1.6129e-01,  4.5111e-01,\n",
      "          1.2678e+00,  1.9698e+00, -1.3986e+00],\n",
      "        [ 1.0459e+00,  6.1831e-02, -2.8123e-01, -8.2985e-01, -7.1830e-01,\n",
      "         -3.4776e-01,  8.8192e-01, -1.6952e+00],\n",
      "        [ 1.0538e+00,  9.5340e-01,  1.1411e+00, -8.5860e-01, -3.1489e-01,\n",
      "          1.6556e+00, -1.0655e+00, -5.3791e-01],\n",
      "        [-1.0360e-03,  7.9850e-02, -1.2795e+00, -2.9511e-01,  2.4285e-01,\n",
      "         -8.4747e-03, -3.9911e-01,  5.8132e-01],\n",
      "        [-3.1552e-01, -2.7018e-01,  5.2580e-01,  2.0464e+00, -2.9935e-01,\n",
      "          5.6005e-01,  8.9928e-01, -2.6875e-01],\n",
      "        [-1.3401e-01, -1.8950e+00, -7.1632e-01, -4.3917e-01,  6.6857e-01,\n",
      "          1.0991e+00,  2.5450e-02, -6.5047e-01],\n",
      "        [-1.1151e+00, -2.3036e-01,  1.1850e-01,  5.9991e-01, -1.6092e+00,\n",
      "         -1.2175e+00,  5.1324e-01,  5.3403e-01]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-2.5870, -0.6140,  0.9152,  0.3614,  1.4714, -0.5973,  0.1084, -0.4063],\n",
      "        [ 0.3114, -2.1958, -0.2169,  0.2235, -1.1157, -1.0754, -0.8815,  1.4612],\n",
      "        [ 0.2684,  0.0608, -0.4062, -1.4609,  1.2044,  0.4708, -0.8580, -0.9521],\n",
      "        [-0.5150,  0.1575,  1.2327,  0.9435,  2.0821, -0.5145,  0.1052, -0.1935],\n",
      "        [ 0.2040,  0.5721, -0.6184, -0.8243, -0.8896, -0.7007,  1.1841, -0.5040],\n",
      "        [-0.4208,  0.8318,  0.8244, -0.4903, -0.3766,  0.3518,  0.0879,  0.4672],\n",
      "        [ 0.5319,  1.4775,  0.0127, -1.3821,  0.9384, -0.7240,  0.7315,  1.1859],\n",
      "        [ 0.3148,  0.5941,  0.2322, -0.7639, -2.0107,  0.7250, -0.5696, -1.5423],\n",
      "        [ 1.4811,  0.5932,  0.0063, -0.5517, -0.0627, -0.9319,  1.2726,  0.6704]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 构造Embedding\n",
    "# 总共有1-8 8个可能的单词，但是还有一个负责padding的0，所以要加一个0，也就是8+1=9\n",
    "# 相当于就是一个9*8的矩阵\n",
    "src_embedding_table = nn.Embedding(max_num_src_words+1,model_dim)\n",
    "tgt_embedding_table = nn.Embedding(max_num_src_words+1,model_dim)\n",
    "print('==========构造嵌入表==========')\n",
    "print(src_embedding_table.weight)\n",
    "print(tgt_embedding_table.weight)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========根据嵌入表将seq转换为向量==========\n",
      "tensor([[1, 5, 0, 0, 0],\n",
      "        [6, 2, 2, 7, 0]])\n",
      "tensor([[[-2.1713e+00,  5.4870e-03,  2.9440e-02,  2.2757e-01,  4.2504e-01,\n",
      "           1.7870e+00,  3.9924e-01, -1.4182e+00],\n",
      "         [-1.0360e-03,  7.9850e-02, -1.2795e+00, -2.9511e-01,  2.4285e-01,\n",
      "          -8.4747e-03, -3.9911e-01,  5.8132e-01],\n",
      "         [-1.4095e-01,  7.1341e-02,  5.8432e-01, -2.9907e-01, -1.7384e+00,\n",
      "          -4.7295e-01,  2.8286e-01, -3.2231e-01],\n",
      "         [-1.4095e-01,  7.1341e-02,  5.8432e-01, -2.9907e-01, -1.7384e+00,\n",
      "          -4.7295e-01,  2.8286e-01, -3.2231e-01],\n",
      "         [-1.4095e-01,  7.1341e-02,  5.8432e-01, -2.9907e-01, -1.7384e+00,\n",
      "          -4.7295e-01,  2.8286e-01, -3.2231e-01]],\n",
      "\n",
      "        [[-3.1552e-01, -2.7018e-01,  5.2580e-01,  2.0464e+00, -2.9935e-01,\n",
      "           5.6005e-01,  8.9928e-01, -2.6875e-01],\n",
      "         [-9.5577e-01, -3.4160e-01,  2.2756e-01, -1.6129e-01,  4.5111e-01,\n",
      "           1.2678e+00,  1.9698e+00, -1.3986e+00],\n",
      "         [-9.5577e-01, -3.4160e-01,  2.2756e-01, -1.6129e-01,  4.5111e-01,\n",
      "           1.2678e+00,  1.9698e+00, -1.3986e+00],\n",
      "         [-1.3401e-01, -1.8950e+00, -7.1632e-01, -4.3917e-01,  6.6857e-01,\n",
      "           1.0991e+00,  2.5450e-02, -6.5047e-01],\n",
      "         [-1.4095e-01,  7.1341e-02,  5.8432e-01, -2.9907e-01, -1.7384e+00,\n",
      "          -4.7295e-01,  2.8286e-01, -3.2231e-01]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[3, 5, 1, 3, 0],\n",
      "        [3, 7, 6, 0, 0]])\n",
      "tensor([[[-0.5150,  0.1575,  1.2327,  0.9435,  2.0821, -0.5145,  0.1052,\n",
      "          -0.1935],\n",
      "         [-0.4208,  0.8318,  0.8244, -0.4903, -0.3766,  0.3518,  0.0879,\n",
      "           0.4672],\n",
      "         [ 0.3114, -2.1958, -0.2169,  0.2235, -1.1157, -1.0754, -0.8815,\n",
      "           1.4612],\n",
      "         [-0.5150,  0.1575,  1.2327,  0.9435,  2.0821, -0.5145,  0.1052,\n",
      "          -0.1935],\n",
      "         [-2.5870, -0.6140,  0.9152,  0.3614,  1.4714, -0.5973,  0.1084,\n",
      "          -0.4063]],\n",
      "\n",
      "        [[-0.5150,  0.1575,  1.2327,  0.9435,  2.0821, -0.5145,  0.1052,\n",
      "          -0.1935],\n",
      "         [ 0.3148,  0.5941,  0.2322, -0.7639, -2.0107,  0.7250, -0.5696,\n",
      "          -1.5423],\n",
      "         [ 0.5319,  1.4775,  0.0127, -1.3821,  0.9384, -0.7240,  0.7315,\n",
      "           1.1859],\n",
      "         [-2.5870, -0.6140,  0.9152,  0.3614,  1.4714, -0.5973,  0.1084,\n",
      "          -0.4063],\n",
      "         [-2.5870, -0.6140,  0.9152,  0.3614,  1.4714, -0.5973,  0.1084,\n",
      "          -0.4063]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "src_embedding = src_embedding_table(src_seq)\n",
    "tgt_embedding = tgt_embedding_table(tgt_seq)\n",
    "print('==========根据嵌入表将seq转换为向量==========')\n",
    "print(src_seq)\n",
    "print(src_embedding)\n",
    "print(tgt_seq)\n",
    "print(tgt_embedding)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "# further：word_embedding是可训练的？"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n",
      "tensor([[   1.,   10.,  100., 1000.]])\n",
      "==========Position Embedding 表的构造==========\n",
      "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "        [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "          9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "        [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "          9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "        [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "          9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
      "        [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
      "          9.9920e-01,  4.0000e-03,  9.9999e-01]])\n"
     ]
    }
   ],
   "source": [
    "# 构造position embedding\n",
    "pos_mat = torch.arange(max_position_len).reshape(-1,1)\n",
    "i_mat = torch.arange(0,model_dim,2).reshape(1,-1)/model_dim\n",
    "i_mat = torch.pow(10000,i_mat)\n",
    "pe_embedding_table = torch.zeros(max_position_len,model_dim)\n",
    "pe_embedding_table[:,0::2] = torch.sin(pos_mat / i_mat)\n",
    "pe_embedding_table[:,1::2] = torch.cos(pos_mat / i_mat)\n",
    "print(pos_mat)\n",
    "print(i_mat)\n",
    "print('==========Position Embedding 表的构造==========')\n",
    "print(pe_embedding_table)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "        [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "          9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "        [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "          9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "        [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "          9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
      "        [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
      "          9.9920e-01,  4.0000e-03,  9.9999e-01]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 将刚刚得到的数组转换为一个nn.Embedding\n",
    "pe_embedding = nn.Embedding(max_position_len,model_dim)\n",
    "pe_embedding.weight = nn.Parameter(pe_embedding_table,requires_grad=True)\n",
    "print(pe_embedding.weight)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "        [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "          9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "        [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "          9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "        [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "          9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
      "        [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
      "          9.9920e-01,  4.0000e-03,  9.9999e-01]], grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "        [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "          9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "        [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "          9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "        [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "          9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
      "        [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
      "          9.9920e-01,  4.0000e-03,  9.9999e-01]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 现在应该传入位置的索引而不是某个词的索引\n",
    "src_pos = torch.arange(max_src_seq_len)\n",
    "tgt_pos = torch.arange(max_tgt_seq_len)\n",
    "print(src_pos)\n",
    "print(tgt_pos)\n",
    "src_pe_embedding = pe_embedding(src_pos)\n",
    "tgt_pe_embedding = pe_embedding(tgt_pos)\n",
    "print(src_pe_embedding)\n",
    "print(tgt_pe_embedding)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2522, 0.3805, 0.7832, 1.1428, 0.0763])\n",
      "tensor([0.1406, 0.1598, 0.2391, 0.3426, 0.1179])\n"
     ]
    }
   ],
   "source": [
    "# softmax演示\n",
    "score = torch.randn(5)\n",
    "prob = F.softmax(score,dim=0)\n",
    "print(score)\n",
    "print(prob)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1944, 0.1969, 0.2050, 0.2125, 0.1910])\n",
      "tensor([2.1414e-10, 5.2918e-09, 1.2466e-04, 9.9988e-01, 2.6381e-12])\n"
     ]
    }
   ],
   "source": [
    "# 是否归一化对softmax后的影响\n",
    "alpha1 = 0.1\n",
    "alpha2 = 5\n",
    "prob1 = F.softmax(score*alpha1,dim=0)\n",
    "prob2 = F.softmax(score*alpha2*alpha2,dim=0)\n",
    "print(prob1)\n",
    "print(prob2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1566, -0.0383, -0.0399, -0.0413, -0.0371],\n",
      "        [-0.0383,  0.1582, -0.0404, -0.0419, -0.0376],\n",
      "        [-0.0399, -0.0404,  0.1630, -0.0436, -0.0392],\n",
      "        [-0.0413, -0.0419, -0.0436,  0.1674, -0.0406],\n",
      "        [-0.0371, -0.0376, -0.0392, -0.0406,  0.1545]])\n",
      "tensor([[ 9.5768e-03, -1.7761e-04, -1.3301e-03, -8.0303e-03, -3.8815e-05],\n",
      "        [-1.7761e-04,  1.8029e-02, -2.5262e-03, -1.5251e-02, -7.3719e-05],\n",
      "        [-1.3301e-03, -2.5262e-03,  1.1863e-01, -1.1422e-01, -5.5208e-04],\n",
      "        [-8.0303e-03, -1.5251e-02, -1.1422e-01,  1.4083e-01, -3.3331e-03],\n",
      "        [-3.8815e-05, -7.3719e-05, -5.5208e-04, -3.3331e-03,  3.9977e-03]])\n"
     ]
    }
   ],
   "source": [
    "def softmax_func(score):\n",
    "    return F.softmax(score,dim=0)\n",
    "\n",
    "jaco_mat1 = torch.autograd.functional.jacobian(softmax_func,score*alpha1)\n",
    "jaco_mat2 = torch.autograd.functional.jacobian(softmax_func,score*alpha2)\n",
    "print(jaco_mat1)\n",
    "print(jaco_mat2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========合法的位置用1表示==========\n",
      "[tensor([1., 1.]), tensor([1., 1., 1., 1.])]\n",
      "==========填充不合法的位置为0==========\n",
      "[tensor([1., 1., 0., 0.]), tensor([1., 1., 1., 1.])]\n",
      "==========拼接好的矩阵==========\n",
      "tensor([[1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1.]])\n",
      "==========扩维后的矩阵==========\n",
      "torch.Size([2, 4, 1])\n",
      "tensor([[[1.],\n",
      "         [1.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]]])\n"
     ]
    }
   ],
   "source": [
    "# 构造encoder的mask，一般而言mask放在softmax中\n",
    "# mask的shape应该是[batch_size,max_src_len,max_src_len],值为1或者为-inf\n",
    "valid_encoder_pos = [torch.ones(L) for L in src_len]\n",
    "print('==========合法的位置用1表示==========')\n",
    "print(valid_encoder_pos)\n",
    "\n",
    "print('==========填充不合法的位置为0==========')\n",
    "for i in range(batch_size):\n",
    "    valid_encoder_pos[i] = F.pad(valid_encoder_pos[i],(0,max(src_len)-src_len[i]))\n",
    "print(valid_encoder_pos)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    valid_encoder_pos[i] = torch.unsqueeze(valid_encoder_pos[i],0)\n",
    "valid_encoder_pos = torch.cat(valid_encoder_pos)\n",
    "print('==========拼接好的矩阵==========')\n",
    "print(valid_encoder_pos)\n",
    "\n",
    "valid_encoder_pos = torch.unsqueeze(valid_encoder_pos,2)\n",
    "print('==========扩维后的矩阵==========')\n",
    "print(valid_encoder_pos.shape)\n",
    "print(valid_encoder_pos)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 4])\n",
      "tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "计算两个tensor的矩阵乘法，torch.bmm(a,b),tensor a 的size为(b,h,w),tensor b的size为(b,w,m) 也就是说两个tensor的第一维是相等的，然后第一个数组的第三维和第二个数组的第二维度要求一样，对于剩下的则不做要求，输出维度 （b,h,m）\n",
    "\n",
    "输出为该矩阵的含义是:当这个句子中的词的数量为2的时候，第一个字和第一个字是有关联的，同理第一个字和第二个字也是有关联的，但是第一个字和第三个字是没有关联的，因为第三个字压根不存在\n",
    "tensor([[[1., 1., 0., 0.],\n",
    "         [1., 1., 0., 0.],\n",
    "         [0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0.]],\n",
    "'''\n",
    "valid_encoder_pos_matrix = torch.bmm(valid_encoder_pos,valid_encoder_pos.transpose(1,2))\n",
    "print(valid_encoder_pos_matrix.shape)\n",
    "print(valid_encoder_pos_matrix)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========转换为非法矩阵==========\n",
      "tensor([[[0., 0., 1., 1.],\n",
      "         [0., 0., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n",
      "==========将非法矩阵转换为bool形式==========\n",
      "tensor([[[False, False,  True,  True],\n",
      "         [False, False,  True,  True],\n",
      "         [ True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True]],\n",
      "\n",
      "        [[False, False, False, False],\n",
      "         [False, False, False, False],\n",
      "         [False, False, False, False],\n",
      "         [False, False, False, False]]])\n",
      "==========查看score值==========\n",
      "tensor([[[-2.1512e+00,  1.3586e+00,  5.5207e-01,  4.6500e-02],\n",
      "         [ 3.6031e-01, -3.7255e-01,  7.7691e-01,  4.7475e-01],\n",
      "         [ 1.7037e-03, -6.1950e-01, -2.4835e-01, -3.8190e-02],\n",
      "         [-6.6644e-01, -2.6302e-01,  3.9302e-01, -2.6971e-01]],\n",
      "\n",
      "        [[ 1.5728e+00,  2.2050e-01, -4.5096e-01, -7.0078e-01],\n",
      "         [ 4.2933e-01, -1.5702e+00,  8.0941e-01,  1.4206e-01],\n",
      "         [-1.9292e+00, -4.0121e-01, -1.7111e+00,  4.5773e-01],\n",
      "         [ 2.0755e-01, -9.7986e-01, -1.2660e+00, -1.1940e+00]]])\n",
      "==========查看mask后的score值==========\n",
      "tensor([[[-2.1512e+00,  1.3586e+00, -1.0000e+09, -1.0000e+09],\n",
      "         [ 3.6031e-01, -3.7255e-01, -1.0000e+09, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "        [[ 1.5728e+00,  2.2050e-01, -4.5096e-01, -7.0078e-01],\n",
      "         [ 4.2933e-01, -1.5702e+00,  8.0941e-01,  1.4206e-01],\n",
      "         [-1.9292e+00, -4.0121e-01, -1.7111e+00,  4.5773e-01],\n",
      "         [ 2.0755e-01, -9.7986e-01, -1.2660e+00, -1.1940e+00]]])\n",
      "==========查看softmax之后的概率值==========\n",
      "tensor([[[0.0290, 0.9710, 0.0000, 0.0000],\n",
      "         [0.6754, 0.3246, 0.0000, 0.0000],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500]],\n",
      "\n",
      "        [[0.6695, 0.1731, 0.0885, 0.0689],\n",
      "         [0.2987, 0.0404, 0.4368, 0.2241],\n",
      "         [0.0564, 0.2599, 0.0701, 0.6136],\n",
      "         [0.5617, 0.1713, 0.1287, 0.1383]]])\n"
     ]
    }
   ],
   "source": [
    "invalid_encoder_pos_matrix = 1-valid_encoder_pos_matrix\n",
    "mask_encoder_self_attention = invalid_encoder_pos_matrix.to(torch.bool)\n",
    "print('==========转换为非法矩阵==========')\n",
    "print(invalid_encoder_pos_matrix)\n",
    "print('==========将非法矩阵转换为bool形式==========')\n",
    "print(mask_encoder_self_attention)\n",
    "\n",
    "score = torch.randn(batch_size,max(src_len),max(src_len))\n",
    "# 这个-1e9如果换成numpy.inf那么softmax之后就会有nan出现\n",
    "masked_score = score.masked_fill(mask_encoder_self_attention,-1e9)\n",
    "print('==========查看score值==========')\n",
    "print(score)\n",
    "print('==========查看mask后的score值==========')\n",
    "print(masked_score)\n",
    "\n",
    "prob3 = F.softmax(masked_score,dim = 2)\n",
    "print('==========查看softmax之后的概率值==========')\n",
    "print(prob3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
